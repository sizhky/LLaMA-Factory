### model
# ----✂︎----------------------------------------------------------
# vllm_max_lora_rank: 256
# model_name_or_path: meta-llama/Llama-3.2-3B-Instruct
# adapter_name_or_path: /home/paperspace/Code/LLaMA-Factory/saves/gommt/llama-3.2-3B-lora-r256
# template: llama3
# ----✂︎----------------------------------------------------------

model_name_or_path: /home/paperspace/Data/oneshot-training/ilsv2-model/field_v2_post5_4604
template: nanonets-ilsv2

# model_name_or_path: /home/paperspace/Code/LLaMA-Factory/saves/gommt/ilsv2.2-finetuned
# template: nanonets-ilsv2
# ----✂︎----------------------------------------------------------

# adapter_name_or_path: /home/paperspace/Code/LLaMA-Factory/saves/gommt/Qwen2.5-3B-oneshot-full
# model_name_or_path: Qwen/Qwen2.5-3B-Instruct
# template: qwen
# ----✂︎----------------------------------------------------------

# model_name_or_path: Qwen/Qwen2.5-3B-Instruct
# template: qwen
infer_backend: vllm
# use_dora: true
# vllm_enforce_eager: true
# infer_dtype: float16
cutoff_len: 8192
vllm_maxlen: 8192

use_unsloth: true
enable_liger_kernel: true
flash_attn: fa2
# rope_scaling: dynamic